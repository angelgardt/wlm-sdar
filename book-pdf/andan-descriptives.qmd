# Описательные статистики {#andan-descriptives}
::: {.lab-chapter .lab-junior}
:::

```{r andan-desc-pkgs, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
theme_set(theme_bw())
library(latex2exp)
```

## Виды статистики {#andan-descriptives-kinds-of-stats}
::: {.lab-chapter .lab-junior}
:::

Напомним себе, что статистика [как набор методов и инструментов] делится на два вида --- описательная статистика и статистика вывода.

- **Описательная статистика (descriptive statistics[^desc-stats-1])** занимается обработкой статистических данных, их наглядным представлением, и собственно описанием через некоторые характеристики.
  - Эти характеристики, количественно описывающие особенности имеющихся данных, называются **описательными статистиками (descriptive statistics[^desc-stats-2])**.
  - *Задача описательной статистики* --- ёмко описать имеющиеся данные и составить на основе этих описаний общее представление о них, а также обнаружить особенности, которые могут повлиять на дальнейший анализ.
- **Статистика вывода (inferential statistics)** занимается поиском ответов на содержательные вопросы, которые мы задаем данным в ходе их анализа в рамках научных и практических исследований.
  - Состоит из двух компонентов --- *тестирования статистических гипотез* и *статистических методов*.

[^desc-stats-1]: Mass (uncountable) noun
[^desc-stats-2]: Countable noun, plural in this case

::: {.callout-note title="Замечание о машинном обучении"}

В названии книги упомянуто «машинное обучение». Иногда его причисляют к статистике, иногда рассматривают отдельно. На самом же деле, статистические методы лежат где-то между статистикой вывода и машинным обучением.

Почему?

Дело в том, что на статистические методы можно смотреть по-разному.

- Если нашей задачей является поиск ответов на **исследовательские** вопросы о закономерностях, о связи каких-либо факторов или влиянии переменных друг на друга, то мы будем смотреть на статистические модели с точки зрения статистики вывода. Это позволит нам находить ответы на интересующие нас вопросы --- причем не важно, говорим мы о научных исследованиях или об исследованиях в индустрии.
- Если перед нами стоит задача хорошо **предсказывать** одни переменные на основании значений других --- например, выдавать рекомендации на Яндекс.Музыке или в Яндекс.Лавке --- то мы будем смотреть на те же статистические модели с точки зрения машинного обучения.

То есть, модели в анализе данных и машинном обучении одни и те же, но то, какую модель мы назовем хорошей и как мы эту «хорошесть» определим, будет отличаться в зависимости от задачи --- *исследовательская* или *предиктивная* --- которая перед нами стоит.

:::



## Меры центральной тенденции {#andan-descriptives-central-tendency}
::: {.lab-chapter .lab-junior}
:::

Итак, мы хотим описать наши данные. Точнее, распределения переменных, которые у нас в данных есть. Хотим сделать это просто и ёмко. Насколько просто и ёмко? Ну, допустим максимально — одним числом. Для этого неплохо подойдет значение переменной, которое лежит *в центре* распределения.

Как мы будем искать, что там в центре распределения? Зависит от [шкалы]() [@stevens46], в которой измерена конкретная переменная (@tbl-scales-cental-tendencies).

|    **Шкала**   |        **Мера центральной тенденции**        |
|:---------------|:---------------------------------------------|
| _Номинальная_  | Мода                                         |
| _Порядковая_   | Медиана                                      |
| _Интервальная_ | Среднее арифметическое                       |
| _Абсолютная_   | Среднее арифметическое, геометрическое и др. |

: Шкалы и меры центральной тенденции {#tbl-scales-cental-tendencies tbl-colwidths="[25,75]"}

Однако есть некоторые нюансы.


### Мода {#andan-descriptives-mode}
::: {.lab-chapter .lab-junior}
:::

Самый простой вариант найти центральную тенденцию --- это определить наиболее часто встречающееся значение переменной. Это значение называется *модой (mode)*.

::: {#def-mode-discrete}
**Мода** [дискретной переменной] --- наиболее часто встречающееся значение данной переменной.
:::

Например, у нас есть следующий ряд наблюдений по какой-то переменной:

$$
\begin{bmatrix}
1 & 3 & 4 & 6 & 3 & 2 & 3 & 3 & 2 & 4 & 1
\end{bmatrix}
$$

Если мы посчитаем, сколько раз встретилась каждое значение переменной и составим таблицу частот, то получим следующее:

$$
\begin{matrix}
\text{Значение} & 1 & 2 & 3 & 4 & 6 \\
\text{Частота}  & 2 & 2 & 4 & 2 & 1
\end{matrix}
$$

Очевидно, что $3$ встречается чаще других значений --- это и есть мода.

Понятно, что если на нашей шкале нет чисел, а есть текстовые лейблы, это ничего не меняет. Пусть у нас есть переменная с кодами аэропортов:

$$
\begin{bmatrix}
\text{DME} & \text{LED} & \text{IST} & \text{AER} & \text{IST} &\text{SVO} & \text{LED} & \text{VKO} & \text{LED} & \text{IST} & \text{IST} & \text{VKO} & \text{AER} & \text{DME}
\end{bmatrix}
$$

$$
\begin{matrix}
\text{Значение} & \text{DME} & \text{LED} & \text{IST} & \text{AER} & \text{SVO} & \text{VKO}\\
\text{Частота}  & 2 & 3 & 4 & 2 & 1 & 2
\end{matrix}
$$

Мода --- $\text{IST}$ (Международный аэропорт Стамбула, İstanbul Havalimanı).

Так мы действуем в случае с эмпирическим распределением. Если нам известна [функция вероятности переменной (probability mass function, PMF)](), то мы можем определить моду, основываясь на ней:

::: {#def-mode-discrete-pmf}
**Мода** [дискретной переменной] --- это значение переменной, при котором её функция вероятности принимает своё максимальное значение.
:::

$$
\text{mode}(X) = \arg \max(\text{PMF}(X)) = \arg \max_{x_i}(\prob (X = x_i)),
$$ {#eq-mode-pmf}

где $X$ --- дискретная случайная величина, $x_i$ --- значение этой случайной величины.

```{r mode-pmf, echo=FALSE}
#| label: fig-mode-pmf
#| fig-cap: "Определение моды с помощью функции вероятности"


tibble(x = 1:10,
       y = c(.01, .03, .07, .1, .1, .15, .2, .1, .09, .15)) |> 
  ggplot(aes(x, y)) +
  annotate(geom = "point", x = 7, y = 0.2, size = 7, shape = 21, color = "darkred", fill = "red", alpha = .5) +
  geom_point() +
  geom_vline(xintercept = 7, color = "darkred", linetype = "dashed") +
  annotate(geom = "text", label = "это максимум функции", 
           x = 8.5, y = 0.2, color = "darkred") +
  annotate(geom = "text", label = "это мода", 
           x = 7, y = 0, color = "darkred") +
  scale_x_continuous(breaks = 1:10) +
  labs(x = "Value", y = "Probability")
```

Окей, мы видим, что *мода отлично считается на дискретных переменных*. А как же быть с непрерывными?

[Напомним себе](), что вероятность того, что непрерывная случайная величина примет своё конкретное значение, равна нулю. Из этого следует, что все значения непрерывной случайное величины уникальны --- каждое повторяется только один раз. Получается, что строить частотную таблицу бессмысленно...

По этой причине **для непрерывных переменных моду не считают**.


#### Мода для непрерывной переменной {#andan-descriptives-mode-contunious}
::: {.lab-chapter .lab-middle}
:::

Да, это так. Действительно, посчитать моду для непрерывной переменной способом, аналогичным тому, что мы увидели выше, не получится. Однако математиков это не остановило.

Если мы посмотрим на [график плотности вероятности]() (probability density function, PDF), который является аналогом PMF для дискретных переменных, мы увидим, что какие-то значения встречаются чаще, а какие-то реже. Что в общем-то логично. Напомним себе, [как это выглядит](), например, для любимого [стандартного] [нормального распределения]():

```{r mode-continuous-data, echo=FALSE}
mode_cont_data <- tibble(x = seq(-4, 4, by = .01), y = dnorm(x))
mode_high_freq <- mode_cont_data |> filter(x > -.5 & x < .5)
mode_low_freq <- mode_cont_data |> filter(x > -2.5 & x < -1.5)
```

```{r mode-continuous-freqs, echo=FALSE}
#| label: fig-continuous-freqs
#| fig-cap: "Частоты интервалов значений непрерывной случайной величины на функции плотности распределения"


ggplot(mode_cont_data,
       aes(x, y)) +
  geom_line() +
  geom_polygon(data = tibble(y = c(0, 0), x = c(.5, -.5)) |> 
                 bind_rows(mode_high_freq),
               fill = "seagreen", alpha = .5) +
  geom_polygon(data = tibble(y = c(0, 0), x = c(-1.5, -2.5)) |>
                 bind_rows(mode_low_freq),
               fill = "royalblue", alpha = .5) +
  annotate(geom = "text", label = "эти значения встречаются часто",
           x = 0, y = .2, angle = 90, color = "darkgreen") +
  annotate(geom = "text", label = "эти значения\nвстречаются реже",
           x = -2.2, y = .15, angle = 90, color = "darkblue") +
  labs(x = "Value", y = "Density")
```

То есть, самые часто встречающиеся значения --- это **пик распределения**. Там и должна быть мода. Визуально это выглядит достаточно справедливо.

Математики так и решили:

::: {#def-mode-continuous}
**Мода** [непрерывной переменной] --- это значение переменной, при котором её функция плотности вероятности достигает локального[^local-max-mode] максимума.
:::

[^local-max-mode]: Здесь в примере локальный максимум функции плотности вероятности на интервале $(-4, \, 4)$ совпадает с глобальным максимумом --- мы об этом знаем, потому что форма распределения нам известна. В случае эмпрического распределения корректнее говорить именно о локальном максимуме, так как глобальный максимум нам не доступен ввиду того, что мы работаем с выборкой.

$$
\text{mode}(X) = \arg \max(\text{PDF}(X)) = \arg \max_{x \in S}f(x),
$$ {#eq-mode-pdf}

гдe $X$ --- непрерывная случайная величина, $x$ --- значение этой случайной величины, $S$ --- имеющаяся выборка значений переменной.

```{r mode-continuous-mode, echo=FALSE}
#| label: fig-continuous-mode
#| fig-cap: "Положение моды на функции плотности [стандартного] нормального распределения"


ggplot(mode_cont_data,
       aes(x, y)) +
  geom_line() +
  geom_polygon(data = tibble(y = c(0, 0), x = c(.5, -.5)) |> 
                 bind_rows(mode_high_freq),
               fill = "seagreen", alpha = .5) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "darkgreen") +
  geom_point(aes(x = 0, y = .4), color = "darkgreen", size = 2) +
  annotate(geom = "text", label = "мода тут",
           x = 0, y = 0, color = "darkgreen") +
  annotate(geom = "text", label = "локальный максимум тут",
           x = 1.5, y = 0.4, color = "darkgreen") +
  labs(x = "Value", y = "Density")
```

Хотя моду для непрерывной переменной вычислить можно, обычно этого не делают, так как достаточно других мер центральной тенденции для описания распределения.

***

::: {.callout-warning title="Take-home: мода"}
- мода --- это значение переменной, которое встречается в выборке чаще всего
- на практике она рассчитывается через построение частотной таблицы
- используется с дискретными (номинальными и порядковыми) переменными
- для непрерывных переменных её рассчитать можно, но обычного этого не делают
:::



### Унимодальные и полимодальные распределения {#andan-descriptives-unimodal-bimodal}
::: {.lab-chapter .lab-junior}
:::

Нормальное распределение, как и ряд других --- биномиальное, отрицательное биномиальное, пуассоновское --- относятся к *унимодальным*. Такие распределения имеют только *одну моду* (см. @fig-norm-mode, @fig-binom-mode, @fig-poiss-mode).

```{r mode-norm-mode, echo=FALSE}
#| label: fig-norm-mode
#| fig-cap: "Нормальное распределение (μ = 2$, σ = 0.5). Пунктирной линией обозначено положение моды."


tibble(x = seq(0, 4, by = .01),
       y = dnorm(x, mean = 2, sd = 0.5)) |> 
  ggplot(aes(x, y)) +
  geom_line() +
  geom_vline(xintercept = 2, linetype = "dashed") +
  labs(x = "Value", y = "Density")
```

```{r mode-binom-mode, echo=FALSE}
#| label: fig-binom-mode
#| fig-cap: "Биномиальное распределение (n = 50, p = 0.3). Пунктирной линией обозначено положение моды."


tibble(x = seq(0, 20, by = 1),
       y = dbinom(x, prob = 0.3, size = 50)) |> 
  ggplot(aes(x, y)) +
  geom_point() +
  geom_vline(xintercept = 15, linetype = "dashed") +
  labs(x = "Value", y = "Probability")
```

```{r mode-poiss-mode, echo=FALSE}
#| label: fig-poiss-mode
#| fig-cap: "Распределение Пуассона (λ = 5.5). Пунктирной линией обозначено положение моды."


tibble(x = seq(0, 20, by = 1),
       y = dpois(x, lambda = 5.5)) |> 
  ggplot(aes(x, y)) +
  geom_point() +
  geom_vline(xintercept = 5, linetype = "dashed") +
  labs(x = "Value", y = "Probability")
```

Это теоретические распределения. С эмпирическими распределениями дело обстоит так же, хотя они обычно менее гладенькие и красивые (см. @fig-mode-norm-sample и @fig-mode-nbinom-sample).

```{r mode-norm-sample, echo=FALSE}
#| label: fig-mode-norm-sample
#| fig-cap: "Эмпирическое распределение, сгенерированное из нормального распределения (μ = 8, σ = 4, n = 100). `set.seed(314)`. Пунктирной линией обозначено положение моды."

set.seed(314)
tibble(x = rnorm(100, mean = 8, sd = 4)) |> 
  ggplot(aes(x = x)) +
  geom_histogram(aes(y = after_stat(density)), 
                 fill = "gray90", binwidth = .5) +
  geom_density() +
  geom_vline(xintercept = 8.5, linetype = "dashed") +
  labs(x = "Value", y = "Density")
```

```{r mode-norm-sample, echo=FALSE}
#| label: fig-mode-nbinom-sample
#| fig-cap: "Эмпирическое распределение, сгенерированное из логнормального распределения (μ = 1.1, σ = 1.39, n = 30). `set.seed(314)`. Пунктирной линией обозначено положение моды."

set.seed(314)
tibble(x = rlnorm(n = 30, meanlog = 1.1, sdlog = 1.39)) |> 
  ggplot(aes(x = x)) +
  geom_histogram(aes(y = after_stat(density)), 
                 fill = "gray90", binwidth = .5) +
  geom_density() +
  geom_vline(xintercept = 1.35, linetype = "dashed") +
  labs(x = "Value", y = "Density")
```

Однако на практике возможны и другие ситуации. Например, такие (@fig-bimodal, @fig-polymodal):

```{r mode-bimodal, echo=FALSE}
#| label: fig-bimodal
#| fig-cap: Бимодальное распределение. Сгенерировано из двух нормальных распределений (μ<sub>1</sub> = 1.5, σ<sub>1</sub> = 0.4, n<sub>1</sub> = 80; μ<sub>2</sub> = 4, σ<sub>2</sub> = 0.5, n<sub>2</sub> = 40). `set.seed(65)`. Пунктирными линиями обозначены положения мод.

set.seed(65)
tibble(x = c(rnorm(80, 1.5, 0.4), rnorm(40, 4, 0.5))) |> 
  ggplot(aes(x)) +
  geom_histogram(aes(y = after_stat(density)), 
                 fill = "gray90", binwidth = .4) +
  geom_density() +
  geom_vline(xintercept = 1.5, linetype = "dashed") +
  geom_vline(xintercept = 4.1, linetype = "dashed") +
  labs(x = "Value", y = "Density")
```

```{r mode-polymodal, echo=FALSE}
#| label: fig-polymodal
#| fig-cap: Полимодальное распределение. Сгенерировано из двух нормальных распределений (μ<sub>1</sub> = 1.5, σ<sub>1</sub> = 0.3, n<sub>1</sub> = 80; μ<sub>2</sub> = 3.4, σ<sub>2</sub> = 0.5, n<sub>2</sub> = 40) и бета-распределения (α = 2, β = 4, n = 50). `set.seed(65)`. Пунктирными линиями обозначены положения мод.

set.seed(65)
tibble(x = c(rnorm(80, 1.5, 0.3), rnorm(40, 3.4, 0.5), rbeta(50, 2, 4))) |> 
  ggplot(aes(x)) +
  geom_histogram(aes(y = after_stat(density)), 
                 fill = "gray90", binwidth = .4) +
  geom_density() +
  geom_vline(xintercept = 0.4, linetype = "dashed") +
  geom_vline(xintercept = 1.45, linetype = "dashed") +
  geom_vline(xintercept = 3.5, linetype = "dashed") +
  labs(x = "Value", y = "Density")
```

В первом случае (@fig-bimodal) мы видим *два локальных максимума* функции плотности вероятности --- такое распределение называется **бимодальным**. Во втором случае (@fig-polymodal) функция плотности вероятности имеет *три локальных максимума* --- такое распределение называется **полимодальным**. Бимональное распределение является частным случаем полимодального распределения.

В прицнипе, пиков может быть и больше, однако при работе с реальными данными чаще всего мы сталкиваемся с бимодальными распределениями.

**Что это значит и что с этим делать?**

Бимодальное распределение сигнализирует нам о **гетерогенности выборки**. Если мы видим два выделяющихся пика, стоит подумать о том, что наша выборка неоднородна и в ней выделяются две подвыборки. Посмотрим на структуру выборки из примера выше (@fig-bimodal-struct):

```{r mode-bimodal-struct, echo=FALSE}
#| label: fig-bimodal-struct
#| fig-cap: "Структура бимодального распределения из @fig-bimodal. Для удобства сопоставления графиков плотностей вероятности по оси ординат отложены частоты."

set.seed(65)
tibble(x = c(rnorm(80, 1.5, 0.4), rnorm(40, 4, 0.5)),
       d = c(rep("A", times = 80), rep("B", times = 40))) -> bimodal_struct_data
gridExtra::grid.arrange(
  bimodal_struct_data |> 
    ggplot(aes(x)) +
    geom_histogram(fill = "gray90", binwidth = .4) +
    geom_density(aes(y = after_stat(count))) +
    geom_vline(xintercept = 1.5, linetype = "dashed") +
    geom_vline(xintercept = 4.1, linetype = "dashed") +
    labs(x = "Value", y = "Count"),
  
  bimodal_struct_data |> 
    ggplot(aes(x)) +
    geom_histogram(aes(fill = d), 
                   alpha = .3, binwidth = .4) +
    geom_density(aes(y = after_stat(count), color = d)) +
    geom_vline(xintercept = 1.5, linetype = "dashed") +
    geom_vline(xintercept = 4.1, linetype = "dashed") +
    guides(color = "none", fill = "none") +
    labs(x = "Value", y = "Count")
)
```

Действительно, наше распределение состоит из двух других распределений, у каждого из которого есть своя мода --- поэтому итоговое распределение получается бимодальным. Конечно, сейчас нам это очень удобно показать, потому что мы знаем, как это распределение генерировалось. Когда же у нас есть реальные данные и мы там наблюдаем такого «верблюда», бывает достаточно сложно сказать, что «пошло не так».

Само по себе распределение не даст нам ответ на вопрос, почему оно бимодальное --- чтобы выяснить причины такого поведения переменной нам потребуются другие данные. Обычно у вас в данных есть «соцдем» --- пол, возраст, сфера и место работы, уровень обрвазования и др. Попробуйте построить распределение с разбиением исследуемой бимодальной переменной по переменным «соцдема». Это, к сожалению, не является рецептом успеха, поскольку причина гетерогенности выборки может и не содержаться в ваших данных, но такое изучение данных станет хорошим показателем того, что вы не просто «забили» на странное распределение своей переменной, а поисследователи возможные его причины.

Если вам удалось найти причины гетерогенности выборки --- допустим, у вас выделяются подвыборки «бакалавры» и «магистры» --- стоит подумать о том, как обойтись с этой переменной в планируемом анализе, так как игнорировать её, по-видимому, нельзя, поскольку она влияет на вариатиность данных.

::: {.callout-tip title="Соцдем лишним не бывает"}
На этапе планирования исследования подумайте о том, чем могут отличаться ваши респонденты или испытуемые между собой, помимо индивидуальных различий.

- Если в эксперименте используете задачу мысленного вращения (mental rotation, [@shepard71]), вполне возможно, испытуемые, работающие в сфере 3D-моделирования или дизайна интерьеров, могут сформировать подвыборку.
- В случае HR-исследования, где фиксируется доход респондента, необходимо записать город, в котором он проживает и/или работает.
- При изучении удовлетворенности городским пространством важными пунктами станут беременность, наличие/отсутствие детей, наличие/отсутствие автомобиля и др.

И так далее. Примеров для каждого случая можно подобрать много.

Стоит ли, скажем, в первом случае сразу исключить из выборки 3D-моделлеров? Зависит. От количества времени и денег на проведение исследования. Однако *как минимум эту информацию надо зафиксировать в данных*. А решить, исключать ли этих респондентов из выборки или нет, можно и позже. Главно об этом написать в отчете/статье, когда будете описывать предобработку данных.
:::

***

::: {.callout-warning title="Take-home: бимодальное распределение"}
- бимодальное распределение намекает на неоднородность данных --- скорее всего, в выборке есть две подвыборки
- необходимо поискать в данных причины этой неодноросности, например, в социально-демографических переменных
- если удалось найти переменную, объясняющую бимодальность, стоит подумать о том, как её учитывать в планируемом анализе
:::



### Медиана {#andan-descriptives-median}
::: {.lab-chapter .lab-junior}
:::

Для номинальной шкалы мода --- это единственно возможная мера центральной тенденции, потому что на этой шкале отсутствует порядок элементов. На других шкалах наблюдения уже можно сортировать по возрастнию или убыванию, поскольку начиная с ранговой (порядковой) шкалы на всех них определена операция сравнения на «больше-меньше».

Возьмем тот же ряд наблюдений, что и в предыдущем разделе:

$$
\begin{bmatrix}
1 & 3 & 4 & 6 & 3 & 2 & 3 & 3 & 2 & 4 & 1
\end{bmatrix}
$$

Отсортируем наблюдения по возрастанию:

$$
\begin{bmatrix}
1  & 1 & 2 & 2 & 3 & 3 & 3 & 3 & 4 & 4 & 6
\end{bmatrix}
$$

Наша задача --- определить центральную тенденцию. Давайте посмотрим, что оказалось в середине отсортированного ряда:


$$
\begin{bmatrix}
1 & 1 & 2 & 2 & 3 & \mathbf{3} & 3 & 3 & 4 & 4 & 6
\end{bmatrix}
$$

Это медиана. В данном случае она равна $3$.

::: {#def-median}
**Медиана (median)** --- это значение, которое располагается на середине отсортированного ряда значений переменной. 
:::

Медиана делит все наблюдения переменной ровно пополам и половина наблюдений оказывается по одну сторону от медианы, а половина --- по другую.

Если число наблюдений нечётное, то всё ясно --- в середине отсортированного ряда будет какое-то значение. А если число наблюдений чётное? Тогда мы попадаем между значениями. 

Возьмем для примера такой вектор наблюдений:

$$
\begin{bmatrix}
14 & 10 & 9 & 16 & 30 & 3 & 25 & 8 & 18 & 7
\end{bmatrix}
$$

Отсортируем:

$$
\begin{bmatrix}
3 & 7 & 8 & 9 & 10 & 14 & 16 & 18 & 25 & 30
\end{bmatrix}
$$

Найдем середину:

$$
\begin{bmatrix}
3 & 7 & 8 & 9 & 10 & | & 14 & 16 & 18 & 25 & 30
\end{bmatrix}
$$

В таком случае в качестве медианы берется среднее между двумя срединными значениями:

$$
\text{median} = \frac{10 + 14}{2} = 12
$$

Итого, формализовать вычисление медианы можно следующим образом:

$$
\text{median}(X) = X(a) =
\cases{
X\left(\frac{n+1}{2}\right), & if  2 | n \\
\dfrac{X(\frac{n}{2}) + X(\frac{n}{2} + 1)}{2}, & otherwise
}
$$ {#eq-median-formula}

где $X$ --- ряд наблюдений случайной величины, $n$ --- число наблюдений, $X(a)$ --- наблюдение с индексом $a$ в отсортированном векторе $X$.

Если мы будем смотреть на медиану с позиции описания распределения, то она будет той самой линией, которая разделит площадь под графиком функции плотности вероятности пополам:

```{r median-norm, echo=FALSE}
#| label: fig-median-norm
#| fig-cap: "Медиана нормального распределения"

tibble(x = seq(-4, 4, by = .01),
       y = dnorm(x)) -> median_norm_data 
median_norm_data |> filter(x >= 0) -> median_norm_upper
median_norm_data |> filter(x <= 0) -> median_norm_lower
median_norm_data |> 
  ggplot(aes(x, y)) +
  geom_line() +
  geom_polygon(data = tibble(y = c(0, 0), x = c(4, 0)) |> 
                 bind_rows(median_norm_upper),
               fill = "seagreen", alpha = 0.5) +
  geom_polygon(data = tibble(y = c(0, 0), x = c(0, -4)) |> 
                 bind_rows(median_norm_lower),
               fill = "royalblue", alpha = 0.5) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  annotate(geom = "text", label = "эти значения\nбольше медианы \n\n 50%", 
           color = "darkgreen", x = 1, y = 0.05) +
  annotate(geom = "text", label = "эти значения\nменьше медианы \n\n 50%", 
           color = "darkblue", x = -1, y = 0.05) +
  annotate(geom = "text", label = "это медиана", 
           x = -0.01, y = 0)
```

При этом форма распределения не имеет значения --- площадь под графиком всегда будет делиться пополам:

```{r median-left-skew, echo=FALSE}
#| label: fig-median-left-skew
#| fig-cap: Медиана распределения с отрицательной асимметрией.

set.seed(115)
tibble(x = seq(0.3, 1, by = .001),
       y = dbeta(x, 7, 1.5)) -> median_leftskew_data

median_leftskew_data |> 
  filter(x >= .845) %>%
  bind_rows(
    tibble(y = c(0, 0),
           x = c(max(median_leftskew_data$x), .845)),
    .) -> median_leftskew_data_upper

median_leftskew_data |> 
  filter(x <= .845) %>%
  bind_rows(
    tibble(y = c(0, 0),
           x = c(.845, min(median_leftskew_data$x))),
    .) -> median_leftskew_data_lower

# DescTools::AUC(median_leftskew_data_upper$x,
#                median_leftskew_data_upper$y)

median_leftskew_data |> 
  ggplot(aes(x, y)) +
  geom_line() +
  geom_polygon(data = median_leftskew_data_lower, 
               aes(x = x, y = y), fill = "royalblue", alpha = .5) +
  geom_polygon(data = median_leftskew_data_upper, 
               aes(x = x, y = y), fill = "seagreen", alpha = .5) +
  geom_vline(xintercept = 0.845, linetype = "dashed") +
  annotate(geom = "text", label = "50%", 
           color = "darkgreen", x = .9, y = 1) +
  annotate(geom = "text", label = "50%", 
           color = "darkblue", x = .8, y = 1) +
  labs(x = "Value", y = "Density")
```

```{r median-right-skew, echo=FALSE}
#| label: fig-median-right-skew
#| fig-cap: Медиана распределения с положительной асимметрией.

set.seed(115)
tibble(x = seq(0, 5, by = .001),
       y = dgamma(x, 2, 2)) -> median_rightskew_data

median_rightskew_data |>
  filter(x >= .85) %>%
  bind_rows(
    tibble(y = c(0, 0),
           x = c(max(median_rightskew_data$x), .85)),
    .) -> median_rightskew_data_upper

median_rightskew_data |>
  filter(x <= .85) %>%
  bind_rows(
    tibble(y = c(0, 0),
           x = c(.85, min(median_rightskew_data$x))),
    .) -> median_rightskew_data_lower

# DescTools::AUC(median_rightskew_data_lower$x,
#                median_rightskew_data_lower$y)

median_rightskew_data |> 
  ggplot(aes(x, y)) +
  geom_line() +
  geom_polygon(data = median_rightskew_data_lower, 
               aes(x = x, y = y), fill = "royalblue", alpha = .5) +
  geom_polygon(data = median_rightskew_data_upper, 
               aes(x = x, y = y), fill = "seagreen", alpha = .5) +
  annotate(geom = "text", label = "50%", 
           color = "darkgreen", x = 1.25, y = .2) +
  annotate(geom = "text", label = "50%", 
           color = "darkblue", x = .5, y = .2) +
  geom_vline(xintercept = 0.85, linetype = "dashed") +
  labs(x = "Value", y = "Density")
```

```{r median-bimodal, echo=FALSE}
#| label: fig-median-bimodal
#| fig-cap: Медиана бимодального распределения.

set.seed(115)
tibble(x = c(rnorm(80, 1.5, 0.4), rnorm(40, 4, 0.5))) -> median_bimodal_data

median_bimodal_data |> 
  ggplot(aes(x)) +
  geom_density() -> g
ggplot_build(g) -> b
b$data[[1]] -> median_bimodal_data_build

median_bimodal_data$x |> median() -> m
# median_bimodal_data_build$x |> median() -> m

median_bimodal_data_build |> 
  select(y, x) |> 
  filter(x >= m) %>% 
  bind_rows(
    tibble(y = c(0, 0),
           x = c(max(median_bimodal_data_build$x), m)),
    .) -> median_bimodal_data_build_upper

median_bimodal_data_build |>
  select(y, x) |> 
  filter(x <= m) %>%
  bind_rows(
    tibble(y = c(0, 0),
           x = c(m , min(median_bimodal_data_build$x))),
    .) -> median_bimodal_data_build_lower

# DescTools::AUC(x = median_bimodal_data_build_upper$x,
#                y = median_bimodal_data_build_upper$y)
# DescTools::AUC(x = median_bimodal_data_build_lower$x,
#                y = median_bimodal_data_build_lower$y)

median_bimodal_data_build |> 
  ggplot(aes(x, y)) +
  geom_histogram(data = median_bimodal_data,
                 aes(x = x, y = after_stat(density)), 
                 fill = "gray90", binwidth = .4) +
  geom_line() +
  geom_polygon(data = median_bimodal_data_build_lower, 
               aes(x = x, y = y), fill = "royalblue", alpha = .5) +
  geom_polygon(data = median_bimodal_data_build_upper, 
               aes(x = x, y = y), fill = "seagreen", alpha = .5) +
  geom_vline(xintercept = m, linetype = "dashed") +
  annotate(geom = "text", label = "50%", 
           color = "darkgreen", x = 2.25, y = .1) +
  annotate(geom = "text", label = "50%", 
           color = "darkblue", x = 1.25, y = .1) +
  labs(x = "Value", y = "Density")
```

***

::: {.callout-warning title="Take-home: медиана"}
- медиану можно расчитать только на шкалах, где задан порядок (ранговая, интервальная, абсолютная)
- медиана делит выборку наблюдений на две равные части
- линия медианы раздели площадь под графиком функции плотности вероятности пополам
:::



### Среднее {#andan-descriptives-mean}
::: {.lab-chapter .lab-junior}
:::

Если наша переменная измерена в самых мощных шкалах --- интервальной или абсолютной --- то нам доступна ещё одна мера центральной тенденции.


#### Арифметическое среднее {#andan-descriptives-arithmetic-mean}
::: {.lab-chapter .lab-junior}
:::

С этим существом все знакомы еще со школы. **Арифметическое среднее (arithmetic mean, mean, average)** считается так:

$$
M_{X} = \bar x = \dfrac{\sum_{i=1}^{n}x_i}{n},
$$

где $\bar X$ --- среднее арифметическое, $x_i$ --- наблюдение в векторе $X$, $n$ --- количество наблюдений.

Ну, то есть всё сложить и поделить на количество того, чего сложили. Изи.

##### Свойства среднего арифметического {#andan-descriptives-arithmetic-mean-features}

* **Если к каждому значению распределения прибавить некоторое число (константу), то среднее увеличится на это же константу.**


$$
M_{X+c} = M_X + c
$$

Вот почему:

$$
M_{X+c} = \frac{\sum_{i=1}^n (x_i + c)}{n} = \frac{\sum_{i=1}^n x_i + nc}{n} = \frac{\sum_{i=1}^n x_i}{n} + c = M_X + c
$$


Иначе говоря, распределение просто сдвинется. Например, если к каждому значению синего распределения прибавить $2$, получится красное:

```{r creating_tibble_for_feature_vis_1, include=FALSE}
smpl1 <- tibble(x1 = seq(-3, 3, by = .001),
               y1 = dnorm(x1),
               x2 = x1 + 2,
               y2 = dnorm(x2, mean = 2))
```

```{r mean_feature_1, echo=FALSE}
smpl1 %>% 
  ggplot() +
  geom_line(aes(x1, y1), color = "blue4") +
  geom_line(aes(x2, y2), color = "red4") +
  geom_vline(xintercept = 0, linetype = "dashed", color = "blue4") +
  geom_vline(xintercept = 2, linetype = "dashed", color = "red4") +
  labs(x = "Value", y = "Density")
```


* **Если каждое значение распределение умножить на некоторое число (константу), то среднее увеличится во столько же раз.**

$$
M_{X \times c} = M_X \times c
$$

Вот почему:

$$
M_{X \times c} = \frac{\sum_{i=1}^n (x_i \times c)}{n} = \frac{c \times \sum_{i=1}^n x_i}{n} = \frac{\sum_{i=1}^n x_i}{n} \times c = M_X \times c
$$

Например, здесь каждое значение синего распределения умножили на $3$ и получили красное:

```{r creating_tibble_for_feature_vis_2, include=FALSE}
smpl2 <- tibble(x1 = seq(-2, 4, by = .001),
               y1 = dnorm(x1, mean = 1),
               x2 = x1 * 3,
               y2 = dnorm(x2, mean = 3, sd = 3))
```

```{r mean_feature_2, echo=FALSE}
smpl2 %>% 
  ggplot() +
  geom_line(aes(x1, y1), color = "blue4") +
  geom_line(aes(x2, y2), color = "red4") +
  geom_vline(xintercept = 1, linetype = "dashed", color = "blue4") +
  geom_vline(xintercept = 3, linetype = "dashed", color = "red4") +
  labs(x = "Value", y = "Density")
```

Тут, правда, явно [что-то ещё](#var_features) произошло, но мы пока этого не знаем. Однако, отметит этот факт.


* **Сумма отклонений от среднего значения равна нулю.**

$$
\sum_{i=1}^n(x_i - M_X) = 0
$$

Элегантное доказательство:

$$
\sum_{i=1}^n(x_i - M_X) = \sum_{i=1}^n x_i - \sum_{i=1}^n M_X = \sum_{i=1}^n x_i - nM_X = \\
= \sum_{i=1}^n x_i - n \times \frac{1}{n} \sum_{i=1}^n x_i = \sum_{i=1}^n x_i - \sum_{i=1}^n x_i = 0
$$

Но можно это осмыслить и более просто графически.

**Отклонение** --- это разность между средним и конкретным значением переменной. И, действительно, так как среднее находится в центре распределения, то часть значений лежит справа, а часть слева. Значит, будут как положительные, так и отрицательные отклонения --- и их сумма в итоге будет равна нулю.

```{r df_polygons, include=FALSE}
poly_left <- smpl1 %>% 
  select(x1, y1) %>% 
  filter(x1 < 0) %>% 
  bind_rows(tibble(x1 = c(0, -3), y1 = c(0, 0)))
poly_right <- smpl1 %>% 
  select(x1, y1) %>% 
  filter(x1 > 0) %>% 
  bind_rows(tibble(x1 = c(3, 0), y1 = c(0, 0)))
```

```{r zero_deviation_sum, echo=FALSE}
smpl1 %>% 
  ggplot() +
  geom_line(aes(x1, y1)) +
  geom_polygon(data = poly_left, aes(x=x1, y=y1), fill="red4", alpha = .5) +
  geom_polygon(data = poly_right, aes(x=x1, y=y1), fill="green4", alpha = .5) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  annotate(geom="text", x = -1, y = .05, label ="отрицательные\nотклонения") +
  annotate(geom="text", x = 1, y = .05, label ="положительные\nотклонения") +
  labs(x = "Value", y = "Density")
```




#### Усеченное среднее {#andan-descriptives-trimmed-mean}
::: {.lab-chapter .lab-junior}
:::

ПРО УСЕЧЕННОЕ СРЕДНЕЕ


Среднее арифметическое не одиноко --- есть и другие. Встретяться они вам примерно нигде --- то есть о-о-о-очень редко и, скорее всего, в каком-то изощрённом виде. Но упомянуть их, пожалуй, стоит.

#### Геометрическое среднее {#andan-descriptives-geometric-mean}
::: {.lab-chapter .lab-middle}
:::

Редко встречается в научных работах, но заради общего представления пусть будет. Поскольку оно считается через умножение, то может быть рассчитано только на абсолютной шкале.

$$
G_{X} = \sqrt[n]{\prod_{i=1}^n x_i} = \Big(\prod_{i=1}^n x_i\Big)^{\tfrac{1}{n}}
$$

#### Квадратичное среднее {#andan-descriptives-quandratic-mean}
::: {.lab-chapter .lab-middle}
:::

> А вот это уже более полезная история. Мы с ним столкнёмся далее, правда под разными масками.

**Квадратичное среднее (quadratic mean, root mean square, RMS)** --- это квадратный корень из среднего квадрата наблюдений. Ничего не понятно, поэтому по порядку.

* есть наблюдение $x_i$
* значит есть и его квадрат $x_i^2$
* мы умеем считать обычно среднее арифметическое, но ведь $x_i^2$ --- это тоже наблюдение, просто в квадрате, так?
* значит можем посчитать среднее арифметическое квадратов наблюдений --- *средний квадрат*

$$
\frac{\sum_{i=1}^n x_i^2}{n}
$$

* норм, а теперь извлечём из этого дела корень --- получим то, что там надо

$$
X_{\mathrm{RMS}} = \sqrt{\frac{\sum_{i=1}^n x_i^2}{n}}
$$

Per se[^per-se] мы его вряд ли ещё когда-то увидим, но пару раз оно внезапно всплывет.

[^per-se]: Per se (лат.) --- «само по себе», «как таковое», «в чистом виде».


#### Гармоническое среднее {#andan-descriptives-harmonic-mean}
::: {.lab-chapter .lab-middle}
:::

> Суперэкзотичный покемон.

$$
H_X = \frac{n \prod_{i=1}^n x_i}{\sum_{i=1}^n (\tfrac{1}{x} \prod_{j=1}^n x_j)} = \frac{n}{\sum_{i=1}^n \tfrac{1}{x_i}}
$$


#### Взвешенное среднее {#andan-descriptives-weighted-mean}
::: {.lab-chapter .lab-junior}
:::

Часто бывает такая ситуация, что нас нужно посчитать среднее по каким-либо имеющимся параметрам, но одни параметры для нас важнее, чем другие. Например, мы хотим вычислить суммарный балл обучающегося за курс на основе ряда работ, выполненных в течение курса, однако мы понимаем, что тест из десяти вопросов с множественном выбором явно менее показателен, чем, например, аналитическое эссе или экзаменационная оценка. Что делать? Взвесить параметры!

Что значит взвесить? Умножить на некоторое число. На самом деле, любое. Пусть мы посчитали, что написать эссе в три абстрактных раза тяжелее, чем написать тест, а сдать экзамен в два раза тяжелее, чем написать эссе. Тогда мы можем присвоить баллу за тест вес $1$, баллу за аналитическое эссе вес $3$, а экзамену --- вес $6$. Тогда итоговая оценка за курс будет рассчитываться следующим образом:

$$
\text{final score } = 1 \cdot \text{test} + 3 \cdot \text{essay} + 6 \cdot \text{exam}
$$

Суперкласс. Однако! Весьма вероятно, что в учебном заведении принята единая система оценки для всех видов работ (ну, скажем, некая абстрактная десятибалльная система в сферическом вакууме). Получается, если и за тест, и за эссе, и за экзамен у студента по 10 баллов, то суммарный балл 100, что, кажется, больше, чем 10. Чтобы вернуться к изначальным границам баллов, нужно моделить суммарный балл на сумму весов параметров:

$$
\text{final score } = \frac{1 \cdot \text{test} + 3 \cdot \text{essay} + 6 \cdot \text{exam}}{1 + 3 + 6}
$$

Кайф! Собственно, это и есть взвешенное среднее. Коэффициенты, на которые мы умножаем значение парамернов, называются *весами параметров*. И в общем виде формула принимает следующий вид.

$$
\bar x = \frac{\sum_{i=1}^n w_i x_i}{\sum_{i=1}^n w_i} = \sum_{i=1}^n w_i' x_i,
$$

где $x_i$ --- значения конкретных параметров, $w_i$ --- веса конкретных параметров, $w_i'$ --- нормированные веса параметров.

Вторая часть формулы показывается нам, что можно облегчить себе вычислительную жизнь, если заранее нормировать веса, то есть разделить каждый коэффициент на сумму коэффициентов:

$$
w_i' = \frac{w_i}{\sum_{i=1}^n w_i}
$$

Тогда сумма коэффициентов будет равна единице. Так чаще всего и поступают, так как тогда коэффициент будет представлять долю, которую весит данный параметр в суммарной оценке. Удобно, практично, красиво.

Взвещенное среднее часто применяется именно во всякого рода ассессментах, и не только образовательных. Например, вы HR-аналитик и оцениваете персонал. Вы аналитически вычисляете веса коэффициентов (допустим, с помощью линейной регрессии), а далее на их основе высчитаете интегральный балл, по которому будете оценивать сотрудников. Это как один из индустриальных примеров.

Также оно применяется, когда в наших данных есть какая-то группировка (например, когорты), при этом группы неравномерны.



### Среднее vs медиана {#andan-descriptives-mean-vs-median}
::: {.lab-chapter .lab-junior}
:::

Помимо того, что среднее и медиана информативны сами по себе, полезно смотреть на их взаимное расположение.

> Сравнивать будем моду, медиану и среднее [арифметическое].

Итак, все три статистики --- мода, медиана и среднее --- описывают центральную тенденцию --- некоторое значение изучаемой нами переменной, вокруг которого собираются другие значения. Но если их три и все они используются, значит между ними должны быть какие-то различия. Посмотрим, какие.

Во-первых, очевидно, что *моду невозможно посчитать для непрерывной переменной*.

<div class="advanced">
<details>
<summary>*Нет, не очевидно*</summary>
Так как вероятность того, что непрерывная случайная величина принимает своё конкретное значение, равна нулю, каждое наблюдение в нашей выборке будет уникально --- встретится ровно *один раз*. Вспомните [посмотрите] пример из предыдущей главы, где мы набирали числа из отрезка. Получается, что мода теряет свой смысл.
</details>
</div>

Во-вторых, *медиану нельзя посчитать на номинальной шкале*. Кстати, почему?

<div class="advanced">
<details>
<summary>*Потому что*</summary>
на номинальной шкале нет отношения порядка между элементами. Помните, на ней нельзя сравнивать на больше-меньше. Поэтому невозможно отсортировать наблюдения, а значит, и найти медиану.
</details>
</div>

В-третьих, *среднее тоже нельзя посчитать на номинальной шкале*.

<div class="advanced">
<details>
<summary>*Можно, но осторожно*</summary>
Вообще, конечно, да --- нельзя, потому что на номинальной шкале не определена операция сложения, входящая в вычисление среднего. Однако если на номинальной шкале есть только *две категории*, которые закодированы `0` и `1`, то посчитать среднее можно. Но что оно будет значить?

Исходный математический смысл среднего явно утерян. Посмотрим на это по-другому: посчитать сумму единиц это всё равно, что посчитать *количество* единиц. То есть, если мы сложим все нули и единицы, то получим количество единиц среди всех наших наблюдений. А разделив количество единиц на количество наблюдений, мы получим *долю единиц* --- то есть долю наблюдений с лейблом `1`.

Вот так вот.
</details>
</div>

В-четвертых, *для дискретной переменной значение среднего арифметического будет не особо осмысленно.* Ну, скажем, странно сказать, что в аудитории в среднем стоят 15.86 столов или в российских семьях в среднем 1.5 ребенка. Конечно, в ряде случаев можно это как-то более-менее водержательно интерпретировать, но это требует усилий, а мы ленивые, поэтому лучше использовать медиану.


**Итого, делаем следующие выводы**:

* для номинальной шкалы пригодна только мода
* для дискретных переменных подходят мода и медиана
   * мода иногда лучше, так как точно всегда будет целым числом
* для непрерывных переменных подходят медиана и среднее


Теперь нам надо разобраться, как будут себя вести меры центральной тенденции в зависимости от *формы распределения*.

```{r central_tendency_sampling, include=FALSE}
set.seed(108)
symm <- sample(
  x = seq(1, 10, 0.5),
  size = 600,
  replace = TRUE,
  prob = c(
    .05,
    .05,
    .07,
    .1,
    .1,
    .15,
    .20,
    .30,
    .35,
    .5,
    .35,
    .30,
    .20,
    .15,
    .1,
    .1,
    .07,
    .05,
    .05
  )
)
asymm_right <- sample(
  x = seq(1, 10, 0.5),
  size = 600,
  replace = TRUE,
  prob = c(
    .1,
    .2,
    .25,
    .4,
    .5,
    .5,
    .4,
    .35,
    .3,
    .25,
    .2,
    .25,
    .2,
    .15,
    .1,
    .1,
    .07,
    .05,
    .05
  )
)
asymm_left <- sample(
  x = seq(1, 10, 0.5),
  size = 600,
  replace = TRUE,
  prob = c(
    .03,
    .05,
    .07,
    .1,
    .15,
    .15,
    .2,
    .2,
    .25,
    .25,
    .3,
    .35,
    .5,
    .5,
    .4,
    .4,
    .25,
    .2,
    .2
  )
)
bimodal <- sample(
  x = seq(1, 10, 0.5),
  size = 600,
  replace = TRUE,
  prob = c(
    .05,
    .05,
    .07,
    .1,
    .1,
    .2,
    .3,
    .35,
    .3,
    .15,
    .1,
    .15,
    .20,
    .40,
    .50,
    .25,
    .1,
    .05,
    .05
  )
)
colors <- c("Mean" = "red4", "Median" = "blue4", "Mode" = "green4")
```

**На симметричном распределении мода, медиана и среднее совпадают** [или, по крайней мере, находятся очень близко друг к другу]. Здесь и далее: красная линия --- среднее, синяя --- медиана, зелёная --- мода.


```{r central_tendency_symm, echo=FALSE}
ggplot(NULL, aes(symm)) +
  geom_histogram(aes(y = ..density..), alpha =.5, binwidth = .5) +
  geom_density() +
  geom_vline(xintercept = mean(symm), color = colors['Mean']) +
  geom_vline(xintercept = median(symm), color = colors['Median']) +
  # geom_vline(xintercept = mode(symm)-0.04, color = colors['Mode']) +
  labs(x = 'Value',
       y = 'Density')
```

**На асимметричном распределении мода [практически] в пике.** Практически, потому что функция плотности вероятности [черная линия на графике] на всегда точно аппроксимирует (в данном случае то же, что и сглаживает) эмпирическое распределение. На картинке ниже мы видим, что на гистограмме мода --- самый высокий столбик, что и показывает нам зелёная линия, которой обозначена мода. Однако при сглаживании гистограммы пик немного съехал, и мода оказалась не совсем в вершине графика функции плотности вероятности.

Вообще-то это нормально, потому что мода для непрерывной величины, которую мы и визуализируем с помощью графика плотности, либо не может быть посчитана вовсе, либо --- если так получилось, и у нас все же есть повторяющиеся значения --- не слишком хорошая мера центральной тенденции. В целом, и на симметричном распределении мода тоже может находиться немного в стороне от пика.

**На асимметричном распределении медиана и среднее смещены в сторону хвоста. Среднее смещено сильнее медианы.** Это связано с тем, что медиана зависит только от количества наблюдений, а среднее ещё и от самих значений. На картинке ниже пример для распределения с *правосторонней* асимметрии (потому что хвост справа) --- среднее (красная линия) *правее* медианы (синяя линия).


```{r central_tendency_asymm_right, echo=FALSE}
ggplot(NULL, aes(asymm_right)) +
  geom_histogram(aes(y = ..density..), alpha =.5, binwidth = .5) +
  geom_density() +
  geom_vline(xintercept = mean(asymm_right), color = colors['Mean']) +
  geom_vline(xintercept = median(asymm_right), color = colors['Median']) +
  # geom_vline(xintercept = mode(asymm_right), color = colors['Mode']) +
  labs(x = 'Value',
       y = 'Density')
```

А это пример для распределения с *левосторонней* асимметрией (так как хвост слева) --- среднее (красная линия) *левее* медианы (синяя линия).


```{r central_tendency_asymm_left, echo=FALSE}
ggplot(NULL, aes(asymm_left)) +
  geom_histogram(aes(y = ..density..), alpha =.5, binwidth = .5) +
  geom_density() +
  geom_vline(xintercept = mean(asymm_left), color = colors['Mean']) +
  geom_vline(xintercept = median(asymm_left), color = colors['Median']) +
  # geom_vline(xintercept = mode(asymm_left), color = colors['Mode']) +
  labs(x = 'Value',
       y = 'Density')
```

Для того, чтобы лучше разобраться с тем, как большие и малые значения влияют на моду и медиану посмотрим такой пример. Пусть у нас есть оценки за выпускную квалификационную работу. Например, такие:

```{r marks_creating, include=FALSE}
marks <- c(6, 7, 7, 8, 8)
```
```{r marks_vector}
marks
```

Посчитаем медиану и среднее:

```{r}
median(marks)
mean(marks)
```

Среднее $7.2$ округлиться до $7$, то есть можно считать, что среднее и медиана совпали. Ну, ок.

Но в комиссии сидят два требовательных доктора наук, которые поставили оценки, сильно отличающиеся от остальных:

```{r marks_creating_2, include=FALSE}
marks <- c(6, 7, 7, 8, 8, 3, 4)
```
```{r marks_vector_2}
marks
```

Посчитаем медиану и среднее теперь:

```{r}
median(marks)
mean(marks)
```

Медиана осталась на месте --- всё ещё $7$. А вот среднее $6.1$ округлится до $6$. Казалось бы, это немного, но в смысле оценок --- это прилично, и может сильно повлиять на GPA.

Итого, среднее более чувствительно к нетипичным значениям (очень большим или очень малым).


Есть ещё один интересный вариант распределений --- **бимодальные**. Значит ли, что у этого распределения две моды? Не всегда. Посмотрим пример ниже:

```{r central_tendency_bimodal, echo=FALSE}
ggplot(NULL, aes(bimodal)) +
  geom_histogram(aes(y = ..density..), alpha =.5, binwidth = .5) +
  geom_density() +
  geom_vline(xintercept = mean(bimodal), color = 'red4') +
  geom_vline(xintercept = median(bimodal), color = 'blue4') +
  # geom_vline(xintercept = mode(bimodal), color = 'green4') +
  labs(x = 'Value',
       y = 'Density')
```

Мы видим, что на графике есть два пика, однако строго математически мода одна (зеленая линия) --- и она в более высоком пике. Это логично, ибо там самые часто встречающиеся значения.

<div class="advanced">
И все жё содержательно мы не можем пренебречь вторым пиком. Почему нам он важен? Обычно бимодальное распределение --- это повод задуматься о том, что наша выборка неоднородна. *Бимодальное распределение как бы сложено из двух с центрами в двух пиках.* То есть в нашей выборке как будто бы *две подвыборки*, которые обладают разными распределениями интересующего нам признака.

Что с этим делать? Хорошо всегда иметь в данным какие-либо дополнительные переменные --- как минимум соцдем --- чтобы мы могли по данным попытаться предположить, какую группировку мы могли забыть учесть при планировании исследования.
</div>

Со средним и медианой происходит примерно то же, что и в случае асимметричного распределения. Второй пик смещает к себе обе меры центральной тенденции, причем среднее вновь сильнее, чем медиану.



## Меры разброса {#andan-descriptives-variability}
::: {.lab-chapter .lab-junior}
:::

Итак, мы разобрались с мерами центральной тенденции. Однако для описания распределения их оказвается недостаточно. Почему?

## Зачем нужны меры разброса {#why_we_need_variation}

Посмотрим на несколько распределений:

```{r distributions_with_the_same_means_generation}
set.seed(123)
tibble(id = 1:100,
       x1 = rnorm(100, mean = 2, sd = 1),
       x2 = rnorm(100, mean = 2, sd = 3),
       x3 = rnorm(100, mean = 2, sd = 0.5)) -> rnorm_three
```

```{r creating_labeller, include=FALSE}
la <- c(x1 = "Variable 1", x2 = "Variable 2", x3 = "Variable 3")
```

```{r distributions_with_the_same_means_vis, echo=FALSE}
rnorm_three %>% 
  pivot_longer(cols = c("x1", "x2", "x3")) %>% 
  ggplot(aes(value)) +
  geom_histogram(binwidth = .5) +
  facet_wrap(~ name,
             labeller = labeller(name = la)) +
  labs(x = "Value",
       y = "Count") +
  scale_x_continuous(breaks = -5:15)
```

Методом пристального взгляда можно установить, что у всех распределений одинаковые средние:

```{r distributions_with_the_same_means_mean, echo=FALSE}
rnorm_three %>% 
  pivot_longer(cols = c("x1", "x2", "x3")) %>% 
  ggplot(aes(value)) +
  geom_histogram(binwidth = .5) +
  geom_vline(xintercept = 2, size = 1) +
  facet_wrap(~ name,
             labeller = labeller(name = la)) +
  labs(x = "Value",
       y = "Count") +
  scale_x_continuous(breaks = -5:15)
```

Однако мы видим, что значения по-разному группируются вокруг среднего. Как они группируются --- плотно, как на третьем рисунке, или не особо, как на втором --- можно описать с помощью **мер разброса**, или **мер вариативности**.


### Основные характеристики статистических данных {#key_features_of_data}

Вообще если посмотреть на это более свысока, то необходимость описания разброса определяется тем, что статистические данные обладают двумя ключевыми особенностями --- *неопределенностью* и *вариативностью*.

* **Неопределённость** нам говорит о том, что мы не знаем, что именно мы получим в результате наших измерений для конкретной выборки. В том числе потому, что мы работаем на просторах случайных величин.
* **Вариативность** означает, что наши данные будут различатся ещё и от респондента к респонденту. И между выборками тоже. Здесь и ошибка измерения, и различные смешения и ещё куча всего.

Более того, вариативность настолько важна, что она входит в расчёт любого статистического критерия. Именно вариативность --- а не центральная тенденция --- позволяет нам сделать вывод о том, что наши выборки различаются (или нет).



### Минимум, максимум, размах {#andan-descriptives-range}
::: {.lab-chapter .lab-junior}
:::

Начнем с самого простого. Как наиболее просто описать вариативность? Мы работаем с выборкой, а в выборке, как известно, ограниченное число наблюдений. А если оно ограниченое, значит среди них точно есть наибольшее --- *максимальное* --- и наименьшее --- *минимальное*.

Допустим, мы открыли ведомость по «Анатомии и физиологии ЦНС» некоторой академической группы и пронаблюли следующее:

```{r anat_marks_gen, include=FALSE}
set.seed(123)
anat_marks <- sample(3:10, 
                     size = 30,
                     replace = TRUE, 
                     prob = c(0.2, 0.5, 0.8, 0.7, 0.7, 0.5, 0.3, 0.2))
```

```{r anat_marks}
anat_marks
```

Мы можем посчитать минимальное и максимальне значение по этому ряду наблюдений:

```{r min_max_anat_marks}
min(anat_marks)
max(anat_marks)
```

Получается, что оценки варьируются от $3$ до $10$. Ну, приемлемо. Разница между максимальным и минимальным значением называется **размах (range)**:

$$
\mathrm{range}(X) = \max(X) - \min(X)
$$

Правда вот функция `range` в `R` вернёт не само значение размаха, а минимальное и максимальное значение. Ну, ладно.

```{r range_anat}
range(anat_marks)
```

И вот мы преисполнившиеся идёт описывать вариативность переменной с помощью размаха, но обнаруживаем в другой ведомости этой же группы (по «Введению в психологию») вот что:

```{r intro_to_psy_gen, include=FALSE}
set.seed(8)
intro_psy_marks <- sample(3:10, 
                     size = 30,
                     replace = TRUE, 
                     prob = c(0.1, 0.3, 0.3, 0.8, 0.8, 0.9, 0.3, 0.2))
```

```{r intro_to_psy_marks}
intro_psy_marks
```

Размах вроде как такой же:

```{r}
range(intro_psy_marks)
```

Значит ли это, что вариативность одинаковая?

Нарисуем.

```{r range_problem, echo=FALSE}
tibble(Anatomy = anat_marks,
       Intro = intro_psy_marks) %>%
  pivot_longer(cols = c("Anatomy", "Intro")) %>% 
  ggplot(aes(value)) +
  geom_bar() +
  facet_grid(name ~ .) +
  scale_x_continuous(breaks = 3:10) +
  scale_y_continuous(breaks = 1:10)
```

Кажется, что вариативность различна. Распределение оценок по «Анатомии и физиологии ЦНС» более равномерное, в то время как оценки по «Введению в психологию» активнее группируются где-то в середине.

Штош, размах хоть и дает нам некоторую информацию о вариативности, нам этого маловато. Будем искать другие меры разброса.



### Среднее абсолютное отклонение {#andan-descriptives-average-absolute-deviation}
::: {.lab-chapter .lab-middle}
:::

#### Среднее абсолютное отклонение от среднего {#andan-descriptives-mean-absolute-deviation-around-the-mean}
::: {.lab-chapter .lab-middle}
:::

#### Среднее абсолютное отклонение от медианы {#andan-descriptives-mean-absolute-deviation-around-the-median}
::: {.lab-chapter .lab-middle}
:::

#### Медианное абсолютное отклонение {#andan-descriptives-median-absolute-deviation}
::: {.lab-chapter .lab-middle}
:::


### Дисперсия {#andan-descriptives-variance}
::: {.lab-chapter .lab-junior}
:::

Хотя описание разброса переменных с помощью квантилей (в частности, квартилей) может дать нам много полезной информации, все же у них есть существенный недостаток: они никак не взаимодействуют с самими значениями нашей переменной.

Действительно, мы делим нашу сортированную выборку на равные части, и смотрим, что в эти части попало. Но хотелось бы как-то учесть ещё и сами значения переменной в некотрой числовой мере разброса.

Ну, хорошо. Поступим следующим образом. Мы все ещё хотим узнать, как наши значения группируются вокруг среднего. В предыдущей главе мы уже видели, что наши наблюдения отклоняются от среднего значения --- значит мы можем посчитать отклонение для каждого наблюдения:

$$
\bar x - x_i
$$

Окей. Если мы сложим все отклоненияи и поделим на их количество (которое равно количеству наблюдений), то мы получим среднее отклонение, да?

$$
\frac{1}{n} \sum_{i=1}^n \bar x - x_i
$$

Да. Однако есть одна проблема. В прошлой главе мы [выяснили](#mean_features), что сумма отклонений от среднего равна нулю, а значит и среднее отклонение также будет равно нулю.

Хорошо. Но отрицательные значения ведь можно победить! Есть два пути:

* **Модуль.** Преимущество первого в том, что размерность величины разброса остается той же, что и у измеряемой переменной.
* **Квадрат.** Преимущество второго в том, что сильные отклонения будут оказывать более сильное влияние на окончательное значение статистики, в то время как для первого малые и большие отклонения равноценны.

Второй путь на практике оказывается полезнее, так как мы хотим, чтобы сильно отличающиеся наблюдения вносили вклад в меру разброса.

Возведя отклонения в квадрат, получим формулу **дисперсии (вариации, variation)**:

$$
D(X) = \mathrm{var}(X) = \sigma^2 = \frac{1}{n} \sum_{i=1}^n (\bar x - x_i)^2
$$

Гениально.

Не совсем. Формула, которую мы получили, пригодна для расчета **дисперсии генеральной совокупности** --- на выборке же она будет давать неточную оценку.

Чтобы получить точную (несмещенную) оценку **дисперсии по выборке**, нам нужно исправить знаменатель дроби --- вместо $n$ использовать $n-1$:

$$
s^2 = \frac{1}{n-1} \sum_{i=1}^n (\bar x - x_i)^2
$$

Но почему?

#### Степени свободы {#andan-descriptives-df}

Во всём виновата выборка.

Взглянем на формулу дисперсии: в неё входит среднее арифметическое. То есть для того, чтобы рассчитать дисперсию на выборке, *сначала* нам необходимо *на этой же выборке* рассчитать среднее. Тем самым, мы как бы «фиксируем» нашу выборку этим средним значением --- у значений нашего распределения становится меньше свободы для варьирования. Теперь свободно варьироваться могут $n-1$ наблюдение, так как последнее всегда будет возможно высчитать, исходя из среднего значения. По этой причине нам необходимо корректировать исходную формулу расчета дисперсии.

А что если не корректировать?

Мы стремимся к тому, чтобы наши расчеты на выборке достаточно точно [на столько, на сколько это возможно] отражали то, что происходит в генеральной совокупности. Математики-статистики выяснили, что та оценка, которая хорошо подходит для расчета дисперсии генеральной совокупности, при применении на выборке даёт *смещенные* оценки. То есть оценка выборочной дисперсии по формуле дисперсии для генеральной совокупности содержит в себе *смещение* --- некоторую систематическую ошибку. Это нехорошо.

К концепту степеней свободы мы ещё неоднократно вернемся. Сейчас хотелось бы, чтобы сформировалось какое-то минимальное более-менее освязаемое понимание того, почему они вообще нам нужны. Если на основе предыдущих абзацев раздела этого сделать не получилось, то давайте попробуеи воспользоваться следующим рассуждением.

На выборке происходят некоторые статистические преколы, которые несколько портят нам жизнь, и нам их неободимо учесть, чтобы адекватно оценивать то, что происходит в генеральной совокупности. В частности, нам необходимо учитывать количество степеней свободы, которое есть в нашей выборке. Для расчета выборочной дисперсии оно равно $n-1$, так как мы для того, чтобы рассчитать дисперсию по выборке, нам сначала *по той же самой выборке* надо рассчитать *ещё одну* оценку --- среднее арифметическое. Этот расчет заберет одну степень свободы у нашей выборки.

#### Дисперсия генеральной совокупности {#andan-descriptives-population-variance}
::: {.lab-chapter .lab-junior}
:::

#### Дисперсия выборки {#andan-descriptives-sample-variance}
::: {.lab-chapter .lab-junior}
:::

### Стандартное отклонение {#andan-descriptives-standard-deviation}
::: {.lab-chapter .lab-junior}
:::

И вот мы получили невероятное! У нас есть формула расчета меры разброса, которая позволяет учесть сами значения переменной! Ну не чудо ли!

Чудо, конечно, однако есть некоторая проблема. Мы возводили отклонения в квадрат. Представим, что мы хотим посчитатить дисперсию роста студентов психфака. Пусть мы измеряли рост в метрах. Отклонения тоже будут в метрах (потому что среднее --- это тоже метры, а если из метров вычитать метры, то мы получим метры). А при возведении метров в квадрат получаются метры в квадрате. Очевидно, что если мы модели квадратные метры на некоторое число ($n$), они все еще останутся метрами в квадрате.

О, нет! А счастье было так близко, так возможно! Получается, мы не можем интерпретировать эту меру разброса? Не сможем даже нарисовать?

Да, но это не очень большая беда. Для того, чтобы вернуться обратно к единицам измерения нашей переменной, нам всего лишь нужно извлечь корень из дисперсии:

$$
\sigma = \sqrt{\sigma^2} = \sqrt{\frac{1}{n} \sum_{i=1}^n (\bar x - x_i)^2}
$$

Мы получили величину, называемую **стандартным отклонением (standard deviation)**. Чем она хороша? Тем, что её размерность совпадает с размерностью нашей переменной. Стандартное отклонение уже может быть достаточно интерпретабельно и хорошо визуализируемо.

Кстати, формула выше, которая [что-то очень напоминает](#quadratic_mean), --- это **стандартное отклонение генеральной совокупности**, потому что под корнем стоит дисперсия генеральной совокупности.

Чтобы посчитать **стандартное отклонение по выборке**, нам надо извлечь корень из выборочной дисперсии:

$$
s = \sqrt{s^2} = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (\bar x - x_i)^2}
$$



### Свойства дисперсии и стандартного отклонения {#andan_descriptives_var_features}

* **Если к каждому значению распределения прибавить некоторое число (константу), то дисперсия не изменится.**

$$
D_{x+c} = D_{x}
$$

Вот почему:

$$
D_{x+c} = \frac{\sum_{i=1}^n \big((\bar x + c) - (x_i + c)\big)^2}{n-1} = \frac{\sum_{i=1}^n \big(\bar x + c - x_i - c\big)^2}{n-1} = \frac{\sum_{i=1}^n \big(\bar x - x_i\big)^2}{n-1} = D_x
$$

* **Если каждое значение распределение умножить на некоторое число (константу), то дисперсия увеличится в** $c^2$ **раз.**

$$
D_{x \times c} = D_{x} \times c^2
$$

Вот почему:

$$
D_{x \times c} = \frac{\sum_{i=1}^n (c\bar x - cx_i)^2}{n-1} = \frac{\sum_{i=1}^n c^2(\bar x - x_i)^2}{n-1} = \frac{c^2 \sum_{i=1}^n (\bar x - x_i)^2}{n-1} = D_x \times c^2
$$

* **Если к каждому значению распределения прибавить некоторое число (константу), то стандартное отклонение не изменится.**

$$
s_{x+c} = s_x
$$

Это следует из свойства дисперсии:

$$
s_{x+c} = \sqrt{D_{x+c}} = \sqrt{D_x} = s_x
$$

Как мы уже видели, распределение просто сдвигается на константу. Например, если к каждому значению синего распределения прибавить $2$, получится красное --- разброс у обоих распределений одинаковый:

```{r creating_tibble_for_feature_sd_vis_1, include=FALSE}
smpl1 <- tibble(x1 = seq(-3, 3, by = .001),
               y1 = dnorm(x1),
               x2 = x1 + 2,
               y2 = dnorm(x2, mean = 2))
```

```{r sd_feature_1, echo=FALSE}
smpl1 %>% 
  ggplot() +
  geom_line(aes(x1, y1), color = "blue4") +
  geom_line(aes(x2, y2), color = "red4") +
  geom_vline(xintercept = 0, linetype = "dashed", color = "blue4") +
  geom_vline(xintercept = 2, linetype = "dashed", color = "red4") +
  labs(x = "Value", y = "Density")
```


* **Если каждое значение распределение умножить на некоторое число (константу), то стандартное отклонение увеличится во столько же раз.**

$$
s_{x \times c} = s_x \times c
$$

Это также следует из свойства дисперсии:

$$
s_{x \times c} = \sqrt{D_{x \times c}} = \sqrt{D_x \times c^2} = s_x \times c
$$

Например, здесь каждое значение синего распределения умножили на $3$ и получили красное --- разброс также увеличился в три раза, поэтому распределение более плоское:

```{r creating_tibble_for_feature_vis_4, include=FALSE}
smpl2 <- tibble(x1 = seq(-2, 4, by = .001),
               y1 = dnorm(x1, mean = 1),
               x2 = x1 * 3,
               y2 = dnorm(x2, mean = 3, sd = 3))
```

```{r sd_feature_2, echo=FALSE}
smpl2 %>% 
  ggplot() +
  geom_line(aes(x1, y1), color = "blue4") +
  geom_line(aes(x2, y2), color = "red4") +
  geom_vline(xintercept = 1, linetype = "dashed", color = "blue4") +
  geom_vline(xintercept = 3, linetype = "dashed", color = "red4") +
  labs(x = "Value", y = "Density")
```




### Квантили {#andan-descriptives-quantiles}
::: {.lab-chapter .lab-junior}
:::

## Квантили {#quantiles}

Возьмем распределение суммарного балла по шкале «Доверие к техническим интеллектуальным системам». Выглядит оно как-то так:

```{r taia_data, include=FALSE}
taia <- read_csv("https://github.com/angelgardt/taia/raw/master/data/taia.csv")
pr_items_0 <- colnames(taia)[8:17]
co_items_0 <- colnames(taia)[18:27]
ut_items_0 <- colnames(taia)[28:39]
fa_items_0 <- colnames(taia)[40:49]
de_items_0 <- colnames(taia)[50:60]
un_items_0 <- colnames(taia)[61:72]
taia_items_0 <- colnames(taia)[8:72]
taia %>% 
  select(id, all_of(taia_items_0)) %>% 
  pivot_longer(all_of(taia_items_0),
               names_to = "subscale",
               values_to = "score") %>% 
  mutate(subscale = str_remove_all(subscale, "[:digit:]{2}") %>% toupper()) %>% 
  group_by(id, subscale) %>% 
  summarise(total_score = sum(score)) %>% 
  ungroup() %>% 
  pivot_wider(id_cols = id,
              names_from = subscale,
              values_from = total_score) %>% 
  # relocate(after = c(id, PR, CO, UT, FA, DE, UN)) %>% 
  mutate(DT = PR + CO + UT + FA + DE + UN) %>% 
  full_join(taia) -> taia
```

```{r taia_score_vis, echo=FALSE}
ggplot(taia, aes(DT)) +
  geom_histogram(binwidth = 3) +
  labs(x = "Trust in Artificial Intelligent Agents",
       y = "Count")
```

Теперь нам понадобится определение квантиля распределения.

**Квантиль** --- это значение переменной, которое *не превышается* с определенной вероятностью (обозначим её $p$). Иначе говоря, *слева* от значения квантиля лежит $p\%$ наблюдений.

Посмотрим на картинки.

*Слева* относительно квантиля-0.05 ($x_{0.05}$) лежит 5% наблюдений:

```{r fifth_vis, echo=FALSE}
ggplot() +
  geom_histogram(data = taia, aes(DT),
                 binwidth = 3) +
  geom_histogram(data = NULL, aes(taia$DT[taia$DT < quantile(taia$DT, .05)]), 
                 binwidth = 3, fill = "green4") +
  geom_vline(xintercept = quantile(taia$DT, .05),
             size = 1.5) +
  annotate(geom = "text", label = "5%", x = 100, y = 5) +
  labs(x = "Trust in Artificial Intelligent Agents",
       y = "Count")
```

*Слева* относительно квантиля-0.68 ($x_{0.68}$) лежит 68% наблюдений:

```{r 68_vis, echo=FALSE}
ggplot() +
  geom_histogram(data = taia, aes(DT),
                 binwidth = 3) +
  geom_histogram(data = NULL, aes(taia$DT[taia$DT < quantile(taia$DT, .68)]), 
                 binwidth = 3, fill = "green4") +
  geom_vline(xintercept = quantile(taia$DT, .68),
             size = 1.5) +
  annotate(geom = "text", label = "68%", x = 160, y = 10) +
  labs(x = "Trust in Artificial Intelligent Agents",
       y = "Count")
```

*Слева* относительно квантиля-0.99 ($x_{0.99}$) лежит 99% наблюдений:

```{r 99_vis, echo=FALSE}
ggplot() +
  geom_histogram(data = taia, aes(DT),
                 binwidth = 3) +
  geom_histogram(data = NULL, aes(taia$DT[taia$DT < quantile(taia$DT, .99)]), 
                 binwidth = 3, fill = "green4") +
  geom_vline(xintercept = quantile(taia$DT, .99) + .5,
             size = 1.5) +
  annotate(geom = "text", label = "99%", x = 175, y = 10) +
  labs(x = "Trust in Artificial Intelligent Agents",
       y = "Count")
```

Итак, мы поняли, а также приняли и осознали, что такое квантиль. Неясно только, как он нам поможет описать вариативность данных.


### Квартили {#quartiles}

Для этого нам пригодятся специально обученные квантили. Оказалось достаточно удобно поделить все наблюдение на *четыре* равные части --- вот так:

```{r quartiles_vis, echo=FALSE}
ggplot() +
  geom_histogram(data = taia, aes(DT),
                 binwidth = 3) +
  geom_histogram(data = NULL, aes(taia$DT[taia$DT < quantile(taia$DT, .26)]), 
                 binwidth = 3, fill = "purple4") +
  geom_histogram(data = NULL, 
                 aes(taia$DT[taia$DT > quantile(taia$DT, .47) & taia$DT < quantile(taia$DT, .75)]), 
                 binwidth = 3, fill = "purple4") +
  geom_histogram(data = NULL, 
                 aes(taia$DT[taia$DT > quantile(taia$DT, .25) & taia$DT < quantile(taia$DT, .51)]), 
                 binwidth = 3, fill = "blue4") +
  geom_histogram(data = NULL, 
                 aes(taia$DT[taia$DT > quantile(taia$DT, .745)]), 
                 binwidth = 3, fill = "blue4") +
  geom_vline(xintercept = quantile(taia$DT, .25),
             size = 1.5) +
  geom_vline(xintercept = quantile(taia$DT, .50),
             size = 1.5) +
  geom_vline(xintercept = quantile(taia$DT, .75),
             size = 1.5) +
  annotate(geom = "text", label = "25%", x = 135, y = 2, color = "white") +
  annotate(geom = "text", label = "25%", x = 160, y = 2, color = "white") +
  annotate(geom = "text", label = "25%", x = 182.5, y = 2, color = "white") +
  annotate(geom = "text", label = "25%", x = 205, y = 2, color = "white") +
  labs(x = "Trust in Artificial Intelligent Agents",
       y = "Count")
```

Значения переменной, которые делят выборку на *четыре* равные части называются **квартили**. Получается, что

* слева от **первого (нижнего) квартиля** ($Q_1$, $x_{0.25}$) лежит 25% наблюдений
* слева от **второго (среднего) квартиля** ($Q_2$, $x_{0.50}$) лежит 50% наблюдений
    * а значит и справа 50% --- получается второй квартиль делит выборку пополам --- это **медиана**
* слева от **третьего (верхнего) квартиля** ($Q_3$, $x_{0.75}$) лежит 75% наблюдений

Четвертый квартиль не используется, потому что это максимальное значение --- слева от него лежит 100% наблюдений.

Кстати, можно также отметить, что первый квартиль --- это медиана нижней (меньшей) половины наблюдений, а третий --- медиана верней (большей) половины наблюдений.

Вот такая вот прикольная история.



### Децили {#deciles}

К слову, делить выборку можно не только на четверти --- можно поделить, скажем, на 10 частей и получить **децили**. Так, слева от *первого дециля* ($x_{0.10}$) лежит 10% наблюдений, а слева от *третьего* ($x_{0.30}$) --- 30%.

Децили встречаются редко (в основном в психометрике), но знать о них полезно.


### Перцентили {#percentiles}

Гораздо чаще встречаются **перцентили** --- значения переменной, которые делят выборку на 100 равных частей. Например, так устроен ваш рейтинг. Только стоит помнить, что в рейтинге отсчет ведется от максимального среднего балла, поэтому если у вас *нулевой перцентиль* ($x_{0.00}$) по программе, значит *выше* вас в рейтинге никого нет. А если ваш перцентиль, скажем, 36-ой ($x_{0.36}$), то выше вас в рейтинге 36% ваших однокурсников, то есть вы все ещё в первой половине рейтинга, что очень неплохо!


### Интерквартильный размах {#iqr}

И --- о, ура! --- мы наконец-то добрались до того, ради чего тут собрались! Зная первый и третий квартили распределения, можно рассчитать **интерквартильный (межквартильный) размах (interquartile range, IQR)**.

$$
\mathrm{IQR}(X) = Q_3(X) - Q_1(X)
$$

Интерквартильный размах --- это разница между третьим и первым квартилем распределения. Эта величина описывает интервал значений признака, в котором лежит 50% наблюдений.

```{r iqr_vis, echo=FALSE}
ggplot() +
  geom_histogram(data = taia, aes(DT),
                 binwidth = 3) +
  geom_histogram(data = NULL, 
                 aes(taia$DT[taia$DT > quantile(taia$DT, .25) & taia$DT < quantile(taia$DT, .75)]), 
                 binwidth = 3, fill = "blue4") +
  geom_vline(xintercept = quantile(taia$DT, .25),
             size = 1.5) +
  geom_vline(xintercept = quantile(taia$DT, .75),
             size = 1.5) +
  annotate(geom = "text", label = "50%", x = 170, y = 2, color = "white") +
  labs(x = "Trust in Artificial Intelligent Agents",
       y = "Count")
```

В данном случае он равен 40:

```{r iqr}
IQR(taia$DT)
```

То есть 50% наблюдений лежит в пределах 40 единиц шкалы.




### Визуализация квартилей. Боксплот {#boxplot}

Отображать квартили на гистограмме, во-первых, совершенно неудобно, а во-вторых, не то чтобы график получается информативный. Для визуализации квартилей придумали специальный тип графика --- **ящик с усами, или боксплот (boxplot)**.

```{r boxplot, echo=FALSE}
taia %>% 
  ggplot(aes(y = DT)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(size = 0))
```

Прикольная ерунда. Научимся его читать.

Значения переменной идут по вертикальной оси (оси ординат). По горизонтальной оси (оси абсцисс) здесь ничего не идет[^x_boxplot]. Жирная линия по середине ящика --- медиана (второй квартиль). Нижняя граница ящика --- первый квартиль, верхняя --- третий. Получается, что границы ящика показывают нам значения, в пределах которых лежит половина наблюдений.

[^x_boxplot]: Но если мы рисуем несколько боксплотов рядом, то на оси `x` будет категориальная переменная.

Нижний ус --- первый квартиль минус полтора межквартильных размаха. Верхний ус --- третий квартиль плюс полтора мехквартильных размаха.

```{r boxplot_annotated, echo=FALSE, warning=FALSE}
taia %>% 
  ggplot(aes(y = DT)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(size = 0)) +
  annotate(geom = "text", x = -.4, y = 170, label = TeX("$Q_2$")) +
  annotate(geom = "text", x = -.4, y = 200, label = TeX("$Q_3$")) +
  annotate(geom = "text", x = -.4, y = 145, label = TeX("$Q_1$")) +
  annotate(geom = "text", x = -.07, y = 95, label = TeX("$Q_1 - 1.5 IQR$")) +
  annotate(geom = "text", x = -.07, y = 245, label = TeX("$Q_3 + 1.5 IQR$"))
```

<div class="advanced">
<details>
<summary>*Замечание*</summary>
Ящик может быть асимметричным --- то есть верхняя его часть (расстояние между медианой и третьим квартилем) и нижняя его часть (расстояние между медианой и первым квартилем) могут быть разными. Это нам говорит об асимметричности распределения. Усы также могут быть неравными, если один из них упирается в максимум / минимум --- тоже по причине асимметричности распределения.
</details>
</div>

Ну, допустим. А что тогда точки?


#### Выбросы {#andan-descriptives-XX-iqr_outliers}

Вообще справедливо было бы задаться вопросом, а зачем нам вообще усы на этом графике? И почему мы прибавляем полтора межквартильных размаха?

Это *один из подходов* к определению нехарактерных значений --- выбросов. При исследовании данных мы часто задаемся вопросом, если ли в наших данных такие значения, которые сильно отличаются от распределения той или иной переменной. Но как определить это самое «сильно»?

Вот один из подходов. Будем считать, что значения, которые укладываются в интервал $(Q_1 - 1.5 \times \mathrm{IQR}, \, Q_3 + 1.5 \times \mathrm{IQR})$, нас устраивают. Все что попадает в этот интервал --- это «нормальные», типичные значения нашей переменной. Те же, которые будут находиться за пределами этого интервала, мы назовем нетипичными, аномальными значениями, или **выбросами**. Эти значения и будут отмечены точками на графике boxplot.

Что с ними делать? Во-первых, содержательной анализировать. Выбросы могут возникнуть по разным причинам. Может быть испытуемый отвлекся на прилетевшего в окно голубя, и у нас в данных появилось время реакции 200 секунд. Такие выбросы мы можем исключить из данных. А возможно в нашу выборку попали какие-то люди, которые, скажем, очень сильно или очень слабо доверяют искусственному интеллекту (как в примере на рисунке). Эти наблюдения необходимо дополнительно проанализировать --- возможно, это представители специфических групп нашей генеральной совокупности (например, программисты-разработчики или люди пенсионного возраста). Анализ принесет нам дополнительную информацию, которую мы могли не учесть при планировании исследования. Крч, думать надо. И собирать побольше данных, чтобы можно было найти содержательную интерпретацию происходящему.


## Сравнение мер разброса {#andan-descriptives_variation_comparison}

Как и разные меры центральной тенденции, разные меры разброса по-своему хороши. Более того, они дружат с мерами центральной тенденции. Так, *с медианой используется мехквартильных размах*, а *со средним арифметическим --- стандартное отклонение*.

Размах подходит для всего сразу. Его стоит рассчитать, чтобы составить самое первое представление в разбросе, о границах измерения изучаемого признака [на нашей выборке].

Стоит также отметить, что все, что мы тут обсуждали, совершенно не годиться для номинативных переменных. Однако у них тоже есть вариативность. Согласитель, что выборка из Питера, Москвы, и Казани более вариативна, чем выборка из Москвы. Аналогом меры разброса для номинальной переменной можно назвать количество уникальных значений этой переменной.



## Асимметрия {#andan-descriptives-skewness}
::: {.lab-chapter .lab-junior}
:::

## Эксцесс {#andan-descriptives-kurtosis}
::: {.lab-chapter .lab-junior}
:::


## Итоги {#andan-descriptives-final}

